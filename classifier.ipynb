{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:/Users/cesco/Desktop/Personal/UPY/9/NLP/proyecto/train.csv', header=None, names=['polarity', 'summary', 'reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cesco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             summary  \\\n",
      "0                     Stuning even for the non-gamer   \n",
      "1              The best soundtrack ever to anything.   \n",
      "2                                           Amazing!   \n",
      "3                               Excellent Soundtrack   \n",
      "4  Remember, Pull Your Jaw Off The Floor After He...   \n",
      "\n",
      "                   cleaned_summary  \\\n",
      "0            stuning even nongamer   \n",
      "1    best soundtrack ever anything   \n",
      "2                          amazing   \n",
      "3             excellent soundtrack   \n",
      "4  remember pull jaw floor hearing   \n",
      "\n",
      "                                          reviewText  \\\n",
      "0  This sound track was beautiful! It paints the ...   \n",
      "1  I'm reading a lot of reviews saying that this ...   \n",
      "2  This soundtrack is my favorite music of all ti...   \n",
      "3  I truly like this soundtrack and I enjoy video...   \n",
      "4  If you've played the game, you know how divine...   \n",
      "\n",
      "                                  cleaned_reviewText  \n",
      "0  sound track beautiful paints senery mind well ...  \n",
      "1  im reading lot reviews saying best game soundt...  \n",
      "2  soundtrack favorite music time hands intense s...  \n",
      "3  truly like soundtrack enjoy video game music p...  \n",
      "4  youve played game know divine music every sing...  \n"
     ]
    }
   ],
   "source": [
    "# Descargar las stopwords de NLTK\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Cargar los datos (reemplazar con la ruta correcta si es necesario)\n",
    "train_df = pd.read_csv('train.csv', header=None, names=['polarity', 'summary', 'reviewText'])\n",
    "\n",
    "# Reemplazar NaN por cadenas vacías en las columnas de texto\n",
    "train_df['summary'] = train_df['summary'].fillna('')\n",
    "train_df['reviewText'] = train_df['reviewText'].fillna('')\n",
    "\n",
    "# Función para preprocesar el texto\n",
    "def preprocess_text(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar caracteres no alfabéticos (números, signos de puntuación, etc.)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Eliminar stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Aplicar el preprocesamiento a las columnas de texto\n",
    "train_df['cleaned_summary'] = train_df['summary'].apply(preprocess_text)\n",
    "train_df['cleaned_reviewText'] = train_df['reviewText'].apply(preprocess_text)\n",
    "\n",
    "# Verificar cómo quedaron las reseñas\n",
    "print(train_df[['summary', 'cleaned_summary', 'reviewText', 'cleaned_reviewText']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (Logistic Regression): [0.86578993 0.86598264 0.86582465 0.86681771 0.86624132]\n",
      "Mean cross-validation score: 0.8661312500000001\n",
      "Clasificación Regresión Logística\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.86      0.87    359759\n",
      "           2       0.86      0.87      0.87    360241\n",
      "\n",
      "    accuracy                           0.87    720000\n",
      "   macro avg       0.87      0.87      0.87    720000\n",
      "weighted avg       0.87      0.87      0.87    720000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Dividir el dataset en entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['cleaned_reviewText'], train_df['polarity'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorizar los textos utilizando TFIDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Crear el clasificador de regresión logística\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Realizar la validación cruzada con K-folds (por ejemplo, 5 folds)\n",
    "cross_val_scores = cross_val_score(log_reg, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Mostrar los resultados de la validación cruzada\n",
    "print(f\"Cross-validation scores (Logistic Regression): {cross_val_scores}\")\n",
    "print(f\"Mean cross-validation score: {np.mean(cross_val_scores)}\")\n",
    "\n",
    "# Entrenar el modelo en todo el conjunto de entrenamiento (ya que K-fold ya hizo la validación)\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predecir en los datos de prueba\n",
    "y_pred = log_reg.predict(tfidf_vectorizer.transform(X_test))\n",
    "\n",
    "# Mostrar el reporte de clasificación\n",
    "print(\"Clasificación Regresión Logística\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cesco\\Miniconda3\\envs\\integrador\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22500/22500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m886s\u001b[0m 39ms/step - accuracy: 0.7077 - loss: 0.4822 - val_accuracy: 0.8817 - val_loss: 0.2795\n",
      "Epoch 2/4\n",
      "\u001b[1m22500/22500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m868s\u001b[0m 39ms/step - accuracy: 0.8827 - loss: 0.2785 - val_accuracy: 0.8883 - val_loss: 0.2672\n",
      "Epoch 3/4\n",
      "\u001b[1m22500/22500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m880s\u001b[0m 39ms/step - accuracy: 0.8881 - loss: 0.2667 - val_accuracy: 0.8915 - val_loss: 0.2602\n",
      "Epoch 4/4\n",
      "\u001b[1m22500/22500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m884s\u001b[0m 39ms/step - accuracy: 0.8921 - loss: 0.2587 - val_accuracy: 0.8931 - val_loss: 0.2575\n",
      "\u001b[1m45000/45000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 6ms/step - accuracy: 0.8929 - loss: 0.2580\n",
      "Accuracy en el fold: 0.8931\n",
      "Epoch 1/4\n",
      "\u001b[1m22500/22500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m911s\u001b[0m 40ms/step - accuracy: 0.7308 - loss: 0.4605 - val_accuracy: 0.8820 - val_loss: 0.2795\n",
      "Epoch 2/4\n",
      "\u001b[1m22500/22500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m862s\u001b[0m 38ms/step - accuracy: 0.8824 - loss: 0.2798 - val_accuracy: 0.8895 - val_loss: 0.2633\n",
      "Epoch 3/4\n",
      "\u001b[1m22500/22500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m873s\u001b[0m 39ms/step - accuracy: 0.8895 - loss: 0.2647 - val_accuracy: 0.8920 - val_loss: 0.2584\n",
      "Epoch 4/4\n",
      "\u001b[1m22500/22500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m886s\u001b[0m 39ms/step - accuracy: 0.8927 - loss: 0.2580 - val_accuracy: 0.8938 - val_loss: 0.2554\n",
      "\u001b[1m45000/45000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 7ms/step - accuracy: 0.8935 - loss: 0.2561\n",
      "Accuracy en el fold: 0.8938\n",
      "Precisión promedio (K-fold): 0.8935\n",
      "\u001b[1m22500/22500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 6ms/step - accuracy: 0.8943 - loss: 0.2537\n",
      "Accuracy en el conjunto de prueba final: 0.8942\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Codificar las etiquetas en números\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(train_df['polarity'])\n",
    "\n",
    "# Tokenización del texto\n",
    "max_words = 5000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_df['cleaned_reviewText'])\n",
    "X_seq = tokenizer.texts_to_sequences(train_df['cleaned_reviewText'])\n",
    "\n",
    "# Padding de las secuencias\n",
    "max_len = 80\n",
    "X_pad = pad_sequences(X_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Dividir el dataset en entrenamiento y prueba final (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inicializar KFold para 5 splits\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "# Listas para almacenar las métricas de cada fold\n",
    "fold_accuracies = []\n",
    "\n",
    "# KFold Cross Validation\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Dividir el dataset en entrenamiento y validación para cada fold\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Crear el modelo DNN\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # EarlyStopping para evitar sobreajuste\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=4, batch_size=64, validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping])\n",
    "\n",
    "    # Evaluar el modelo en el conjunto de validación de este fold\n",
    "    loss, accuracy = model.evaluate(X_val_fold, y_val_fold)\n",
    "    fold_accuracies.append(accuracy)\n",
    "    print(f\"Accuracy en el fold: {accuracy:.4f}\")\n",
    "\n",
    "# Promedio de las precisiones de todos los folds\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "print(f\"Precisión promedio (K-fold): {mean_accuracy:.4f}\")\n",
    "\n",
    "# Evaluación final en el conjunto de prueba\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy en el conjunto de prueba final: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "integrador",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
